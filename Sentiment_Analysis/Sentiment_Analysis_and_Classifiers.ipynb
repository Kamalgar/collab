{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKhQks-Ax7PT"
      },
      "source": [
        "# Sentiment Analysis to predict stock price direction using Classifier models\n",
        "\n",
        "Sentiment Analysis is the use of NLP, text analysis and\n",
        "computational linguistics to determine subjective information. Instead of building our own lexicon to do this, we will use VADER (Valence Aware Dictionary and sEntiment Reasoner), a pre-trained sentiment analysis model included in the NLTK package. We will also use TextBlob, a simple API built upon NLTK, for common NLP tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvvP_QFq-YVv"
      },
      "source": [
        "## 2. Install/import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6GODqk0GhKT-",
        "outputId": "6577c735-ddea-47d6-d7ea-fd709d63ee46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.55)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.7)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TqhEjAwTgWKr",
        "outputId": "c3618557-25d8-496f-deab-171450a874b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as dates\n",
        "import seaborn as sns\n",
        "import seaborn as sns\n",
        "import math\n",
        "import datetime\n",
        "import re\n",
        "import yfinance as yf\n",
        "import nltk\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from datetime import date, timedelta\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.downloader.download('vader_lexicon')\n",
        "from textblob import TextBlob\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTZ3msswBKno"
      },
      "source": [
        "## 3. Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "-l7eoTsDJ7-G",
        "outputId": "d74eb5fa-040f-4556-c6c2-d2d55aca2710"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'azn_article_sentiments_20210105.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e3e5d5adb094>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Show DataFrame of article sentiments data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0marticle_sentiments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'azn_article_sentiments_20210105.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0marticle_sentiments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \"\"\"\n\u001b[1;32m    184\u001b[0m     \u001b[0mexcs_to_catch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'azn_article_sentiments_20210105.pkl'"
          ]
        }
      ],
      "source": [
        "# Show DataFrame of article sentiments data\n",
        "\n",
        "article_sentiments = pd.read_pickle('azn_article_sentiments_20210105.pkl')\n",
        "article_sentiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQJ_zI0Shq4a"
      },
      "outputs": [],
      "source": [
        "# Create copy of DataFrame\n",
        "\n",
        "article_sentiments_azn = article_sentiments.copy()\n",
        "article_sentiments_azn.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQKirUwBGNmo"
      },
      "source": [
        "## 4. Clean news data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECyzvbU2o8Qn"
      },
      "outputs": [],
      "source": [
        "# Append ---newarticle--- to split for NLP\n",
        "\n",
        "article_sentiments_azn['body_text'] = article_sentiments_azn['body_text'].astype(str) + '---newarticle---'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXaIrgU8nUnn"
      },
      "outputs": [],
      "source": [
        "azn_bodytext = article_sentiments_azn['body_text']\n",
        "azn_bodytext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eTi7e9hMucn"
      },
      "outputs": [],
      "source": [
        "pd.set_option(\"display.max_colwidth\", -1)  # to display full text\n",
        "azn_bodytext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEeaBmeulgSi"
      },
      "outputs": [],
      "source": [
        "# Save article_sentiments_azn to txt\n",
        "\n",
        "with open('azn_bodytext_20210105.txt', 'w') as f:\n",
        "    f.write(\n",
        "        azn_bodytext.to_string(header = False, index = False)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r560ehxnIzEN"
      },
      "source": [
        "### Remove spaces in text file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PLBlPegLENV"
      },
      "outputs": [],
      "source": [
        "# first get all lines from file\n",
        "with open('azn_bodytext_20210105.txt', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# remove spaces\n",
        "lines = [line.replace(' ', '') for line in lines]\n",
        "\n",
        "# finally, write lines in the file\n",
        "with open('azn_bodytext_20210105.txt', 'w') as f:\n",
        "    f.writelines(lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb1nSWUEJOGe"
      },
      "source": [
        "### Remove end line breaks from text file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-8UisvrNNmq"
      },
      "outputs": [],
      "source": [
        "# first get all lines from file\n",
        "a_file = open(\"azn_bodytext_20210105.txt\", \"r\")\n",
        "\n",
        "# create variable for string without line breaks\n",
        "string_without_line_breaks = \"\"\n",
        "\n",
        "# iterate over strings\n",
        "for line in a_file:\n",
        "  stripped_line = line.rstrip() # rstrip() method removes any trailing characters - space is the default trailing character to remove\n",
        "  string_without_line_breaks += stripped_line\n",
        "a_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hb8l838NyFh"
      },
      "outputs": [],
      "source": [
        "# finally, write lines in the file\n",
        "with open('azn_bodytext_20210105.txt', 'w') as f:\n",
        "    f.writelines(string_without_line_breaks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSjG-xIbQ4xv"
      },
      "outputs": [],
      "source": [
        "# Read article sentiments data into DataFrame\n",
        "\n",
        "azn_news_df = pd.read_pickle('azn_article_sentiments_20210105.pkl')\n",
        "azn_news_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVXXT8rTSzS3"
      },
      "outputs": [],
      "source": [
        "# Check data types\n",
        "\n",
        "azn_news_df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BHuW701TTwx"
      },
      "outputs": [],
      "source": [
        "# Create copy of DataFrame\n",
        "\n",
        "azn_news_df_new = azn_news_df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ8gO2HPQrCU"
      },
      "source": [
        "### Drop rows without publish date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIG0gL8HabIN"
      },
      "outputs": [],
      "source": [
        "# Drop rows with None in publish_date column\n",
        "\n",
        "azn_news_df_new = azn_news_df_new.replace(to_replace='None', value=np.nan).dropna()\n",
        "azn_news_df_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFhDV3NqQv-f"
      },
      "source": [
        "### Drop rows with duplicate titles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5heOL4cXbO_r"
      },
      "outputs": [],
      "source": [
        "# Dropping all duplicate titles keeping only the first instance\n",
        "\n",
        "azn_news_df_new.drop_duplicates(subset =\"title\",\n",
        "                     keep = 'first', inplace = True)\n",
        "\n",
        "azn_news_df_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGUNECbARAMU"
      },
      "source": [
        "### Set datetime index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzX4pUH2dSNV"
      },
      "outputs": [],
      "source": [
        "# Set string representation of date to datetime index\n",
        "\n",
        "azn_news_df_new['Date'] = pd.to_datetime(azn_news_df_new.publish_date)\n",
        "azn_news_df_new.set_index('Date', inplace=True)\n",
        "azn_news_df_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZXO-EXliJ-H"
      },
      "outputs": [],
      "source": [
        "# Show dateime index of new DataFrame\n",
        "\n",
        "azn_news_df_new.index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfU4xRe4REbK"
      },
      "source": [
        "### Sort in ascending chronological order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzO-zrDdjNaQ"
      },
      "outputs": [],
      "source": [
        "# Sort in chronological order\n",
        "\n",
        "azn_news_df_new = azn_news_df_new.sort_index()\n",
        "azn_news_df_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX-UiTyieYqt"
      },
      "outputs": [],
      "source": [
        "# Save sorted DataFrame\n",
        "\n",
        "azn_news_df_new.to_pickle(\"azn_news_df_new_20210106.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XOFrUVNenWv"
      },
      "outputs": [],
      "source": [
        "azn_news_df_new.to_csv(\"azn_news_df_new_20210106.csv\", sep=',', encoding='utf-8', header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eHNjvTPdqpR"
      },
      "outputs": [],
      "source": [
        "azn_news_df_new.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiVb66JbnzhP"
      },
      "outputs": [],
      "source": [
        "# Create copy of DataFrame\n",
        "\n",
        "azn_news_df_combined = azn_news_df_new.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBn8a7N3ORnh"
      },
      "source": [
        "### Combine articles published on same date\n",
        "\n",
        "Combine all news articles published on same date to get a single score. An alternative method could be to take the mean score of all articles published on the same date."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNxyXjmiomkR"
      },
      "outputs": [],
      "source": [
        "# Combining all the news published on a single date in a separate column\n",
        "\n",
        "azn_news_df_combined['news_combined'] = azn_news_df_combined.groupby(['publish_date'])['body_text'].transform(lambda x: ' '.join(x))\n",
        "\n",
        "azn_news_df_combined"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TCagwDCRYXr"
      },
      "source": [
        "### Drop rows with duplicate dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsTnc-D6sn_i"
      },
      "outputs": [],
      "source": [
        "# Dropping duplicate dates keeping only the first instance\n",
        "\n",
        "azn_news_df_combined.drop_duplicates(subset =\"publish_date\",\n",
        "                     keep = 'first', inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwGBrj2utSsP"
      },
      "outputs": [],
      "source": [
        "# Show DataFrame to check that the number of rows has decreased\n",
        "\n",
        "azn_news_df_combined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdS1hSpzqmMf"
      },
      "outputs": [],
      "source": [
        "# Save combined DataFrame without duplicates\n",
        "\n",
        "azn_news_df_combined.to_csv(\"azn_news_df_combined_20210106.csv\", sep=',', encoding='utf-8', header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIUZn1s2lg_E"
      },
      "outputs": [],
      "source": [
        "azn_news_df_combined = pd.read_csv(\"azn_news_df_combined_20210106.csv\")\n",
        "azn_news_df_combined.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i52j8ispRpjL"
      },
      "source": [
        "### Set datetime index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oy8lWsjvqZx5"
      },
      "outputs": [],
      "source": [
        "# Set string representation of date to datetime index\n",
        "\n",
        "azn_news_df_combined['Date'] = pd.to_datetime(azn_news_df_combined.publish_date)\n",
        "azn_news_df_combined.set_index('Date', inplace=True)\n",
        "azn_news_df_combined"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfE8ssYARxza"
      },
      "source": [
        "## 5. Import historical stock data\n",
        "\n",
        "Import stock data for same period as news data from Yahoo! Finance using yfinance API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glcc8PwlTfr3"
      },
      "outputs": [],
      "source": [
        "azn_stock_df = yf.download(\"AZN.L\", start=\"2014-05-02\", end=\"2021-01-05\")\n",
        "azn_stock_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqXZTfoqSx8k"
      },
      "source": [
        "### Visualise Adjusted Close price and Volume"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gkPw0nraHZ6"
      },
      "outputs": [],
      "source": [
        "# Plot Adjusted Close price and Volume\n",
        "\n",
        "top = plt.subplot2grid((4,4), (0, 0), rowspan=3, colspan=4)\n",
        "top.plot(azn_stock_df.index, azn_stock_df['Adj Close'], label = 'Adjusted Close price')\n",
        "plt.title('AZN.L Adj Close Price')\n",
        "plt.legend(loc=2)\n",
        "bottom = plt.subplot2grid((4,4), (3,0), rowspan=1, colspan=4)\n",
        "bottom.bar(azn_stock_df.index, azn_stock_df[\"Volume\"])\n",
        "plt.title('AZN.L Daily Trading Volume')\n",
        "plt.gcf().set_size_inches(12,8)\n",
        "plt.subplots_adjust(hspace=0.75)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbVDKydaTFfD"
      },
      "source": [
        "## 6. Merge Stock and Sentiment Dataframes on Date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTd0iZM8l_Dq"
      },
      "outputs": [],
      "source": [
        "# Merge data sets on date\n",
        "merge = azn_stock_df.merge(azn_news_df_combined, how='inner', left_index=True, right_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-WY_hiMpMFZ"
      },
      "outputs": [],
      "source": [
        "# Show merged data set\n",
        "merge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0JTgR9rr6Rk"
      },
      "outputs": [],
      "source": [
        "# Save merged DataFrame\n",
        "\n",
        "merge.to_csv(\"azn_news_stock_merge_20210107.csv\", sep=',', encoding='utf-8', header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxKfDtMAvPQX"
      },
      "outputs": [],
      "source": [
        "# Show first row in combined news column\n",
        "\n",
        "merge['news_combined'].iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Lt69R8_viwZ"
      },
      "outputs": [],
      "source": [
        "# Iterate over rows in combined news column\n",
        "\n",
        "for index, row in merge.iterrows():\n",
        "    print (row[\"news_combined\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwx207QNUgDT"
      },
      "source": [
        "### Clean data in combined news column\n",
        "\n",
        "Strip newline escape sequence (\\n), unwanted punctuation and backslashes.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLMIr_BCw6B2"
      },
      "outputs": [],
      "source": [
        "# Create empty list to append cleaned data from combined news column\n",
        "\n",
        "clean_news = []\n",
        "\n",
        "for i in range(0, len(merge[\"news_combined\"])):\n",
        "    clean_news.append(re.sub(\"\\n\", ' ', merge[\"news_combined\"][i]))  # replace n\\ with ' '\n",
        "    clean_news[i] = re.sub(r'[^\\w\\d\\s\\']+', '', clean_news[i]) # remove unwanted punctuation and \\'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txHUTuo61sLQ"
      },
      "outputs": [],
      "source": [
        "# Show first row in cleaned combined news column\n",
        "\n",
        "clean_news[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NmgN8nn9cHo"
      },
      "outputs": [],
      "source": [
        "# Add cleaned news column to merged data set\n",
        "\n",
        "merge['news_cleaned'] = clean_news\n",
        "\n",
        "\n",
        "merge['news_cleaned'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPeqxRYQ9_Fy"
      },
      "outputs": [],
      "source": [
        "# Show head of merged DataFrame\n",
        "\n",
        "merge.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcmZgmzr-UxK"
      },
      "outputs": [],
      "source": [
        "# Save merged DataFrame\n",
        "\n",
        "merge.to_csv(\"azn__merge_cleaned_20210107.csv\", sep=',', encoding='utf-8', header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2Kab6zdWUvj"
      },
      "source": [
        "## 7. Sentiment Analysis\n",
        "\n",
        "TextBlob will be used to get subjectivity and polarity scores for the cleaned and merged news data. Polarity is a float which lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfWqTADC-ejh"
      },
      "outputs": [],
      "source": [
        "# Create function to get subjectivity\n",
        "def getSubjectivity(text):\n",
        "  return TextBlob(text).sentiment.subjectivity\n",
        "\n",
        "# Create function to get polarity\n",
        "def getPolarity(text):\n",
        "  return TextBlob(text).sentiment.polarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KWZRMB3_Biz"
      },
      "outputs": [],
      "source": [
        "# Create new columns\n",
        "merge['subjectivity'] = merge['news_cleaned'].apply(getSubjectivity)\n",
        "merge['polarity'] = merge['news_cleaned'].apply(getPolarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsdORnGUAdDM"
      },
      "outputs": [],
      "source": [
        "# Show new columns\n",
        "merge.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBoGOIUvHoml"
      },
      "outputs": [],
      "source": [
        "# Show shape of DataFrame\n",
        "merge.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Zi19OVHA5eP"
      },
      "outputs": [],
      "source": [
        "# Save DataFrame with subjectivity and polarity scores\n",
        "merge.to_csv(\"azn__merge_cleaned_subj_pol_20210107.csv\", sep=',', encoding='utf-8', header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7owtk0PAgqBR"
      },
      "source": [
        "## 9. Feature engineering\n",
        "\n",
        "We will calculate whether the next day Adjusted Close price increased/held or decreased and label these as 1 and 0 respectively to build and train machine learning classifier models to predict price direction based on sentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QA9-SFdDPfPW"
      },
      "outputs": [],
      "source": [
        "# Create copy of stock data\n",
        "\n",
        "azn_stock_df_label = azn_stock_df.copy()\n",
        "azn_stock_df_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueBiwBRTP76G"
      },
      "outputs": [],
      "source": [
        "# \"1\" when AZN Adj Close value rose or stayed as the same;\n",
        "# \"0\" when AZN Adj Close value decreased.\n",
        "\n",
        "azn_stock_df_label['Adj Close Next'] = azn_stock_df_label['Adj Close'].shift(-1)\n",
        "azn_stock_df_label['Label'] = azn_stock_df_label.apply(lambda x: 1 if (x['Adj Close Next']>= x['Adj Close']) else 0, axis =1)\n",
        "\n",
        "azn_stock_df_label[['Adj Close', 'Adj Close Next', 'Label']].head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gd1qIocVSWXB"
      },
      "outputs": [],
      "source": [
        "# Save DataFrame\n",
        "\n",
        "azn_stock_df_label.to_pickle(\"azn_stock_df_labels_20210107.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ahvq-_IFSmJ1"
      },
      "outputs": [],
      "source": [
        "azn_stock_df_label.to_csv(\"azn_stock_df_label_20210107.csv\", sep=',', encoding='utf-8', header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8OTV3HvSB4_"
      },
      "outputs": [],
      "source": [
        "# Show Adj Close Next and Label with Date\n",
        "\n",
        "azn_stock_df_label_adj_nxt = azn_stock_df_label[['Adj Close Next', 'Label']]\n",
        "azn_stock_df_label_adj_nxt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlqq_8jmW0c8"
      },
      "outputs": [],
      "source": [
        "# Drop NaN row\n",
        "\n",
        "azn_stock_df_label_adj_nxt = azn_stock_df_label_adj_nxt.dropna()\n",
        "azn_stock_df_label_adj_nxt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nu9wVoTGltlB"
      },
      "outputs": [],
      "source": [
        "# Merge DataFrames on date\n",
        "merge2 = azn_stock_df.merge(azn_stock_df_label_adj_nxt, how='inner', left_index=True, right_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQSSPnFiVMJu"
      },
      "outputs": [],
      "source": [
        "# Drop NaN row and show merged DataFrame\n",
        "merge2 = merge2.dropna()\n",
        "merge2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Go_8p40KXFO_"
      },
      "outputs": [],
      "source": [
        "# Save DataFrame\n",
        "merge2.to_csv(\"azn_prices_labels_20210107.csv\", sep=',', encoding='utf-8', header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_iDah7rXQxM"
      },
      "outputs": [],
      "source": [
        "merge2.to_pickle(\"azn_prices_labels_20210107.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7fDIfszXp5e"
      },
      "outputs": [],
      "source": [
        "# Merge next day Adjusted Close price and Label with combined stock data and sentiment DataFrame\n",
        "\n",
        "merge3 = azn_stock_df_label_adj_nxt.merge(merge, how='inner', left_index=True, right_index=True)\n",
        "merge3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VGMJ8PRYdSZ"
      },
      "outputs": [],
      "source": [
        "# Save merged DataFrame\n",
        "\n",
        "merge3.to_csv(\"azn_prices_labels_news_20210107.csv\", sep=',', encoding='utf-8', header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtF20K0sYkAk"
      },
      "outputs": [],
      "source": [
        "merge3.to_pickle(\"azn_prices_labels_news_20210107.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DG4jVv4EqSmE"
      },
      "outputs": [],
      "source": [
        "merge3 = pd.read_pickle(\"azn_prices_labels_news_20210107.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5YJEqBkD6Me"
      },
      "outputs": [],
      "source": [
        "# Collapse data set to keep relevant stock price and sentiment score columns only\n",
        "\n",
        "keep_columns = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'subjectivity', 'polarity', 'compound', 'neg',\t'neu',\t'pos', 'Label']\n",
        "df =  merge3[keep_columns]\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KmjUeHKoUHZ"
      },
      "source": [
        "## 10. Modelling\n",
        "\n",
        "Split the data in feature matrix (X) and target vector (y).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLIcFFZNbSjq"
      },
      "outputs": [],
      "source": [
        "# Create feature data set\n",
        "X = df\n",
        "X = np.array(X.drop(['Label'], 1))\n",
        "\n",
        "# Create target data set\n",
        "y = np.array(df['Label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-L2QrMEonrL"
      },
      "source": [
        "We will split the data into train and test sets to verify predictions. Time series data cannot be split randomly as this would introduce look-ahead bias so the first 80% will be the training set and the last 20% the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZKoXbT3rxyJ"
      },
      "outputs": [],
      "source": [
        "# Split data into 80% training and 20% testing data sets\n",
        "\n",
        "split = int(0.8*len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-Hbcaw6sGcY"
      },
      "outputs": [],
      "source": [
        "X_train = X[0:split]\n",
        "y_train = y[0:split]\n",
        "\n",
        "X_test = X[split:]\n",
        "y_test = y[split:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQfrsz3XsP_H"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcYjN0qicO4L"
      },
      "outputs": [],
      "source": [
        "# Create and train the model\n",
        "model = LinearDiscriminantAnalysis().fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6-TbsFlu_dQ"
      },
      "outputs": [],
      "source": [
        "# Show model's predictions\n",
        "predictions = model.predict(X_test)\n",
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQUXcII8vFnJ"
      },
      "outputs": [],
      "source": [
        "# Show actual values\n",
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12AD0O6VvOWq"
      },
      "outputs": [],
      "source": [
        "# Show model metrics\n",
        "print(classification_report(y_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poqLAGw7pMm5"
      },
      "source": [
        "### Feature scaling\n",
        "\n",
        "We will standardise the data using scikit-learn's preprocessing.scale() algorithm so that it is all on one scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkoD5YHPuM0k"
      },
      "outputs": [],
      "source": [
        "# Standardise X's\n",
        "X_train = scale(X_train)\n",
        "X_test = scale(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACuJx6JfrHdH"
      },
      "source": [
        "### Create function for confusion matrix to visualise performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkXXe_LNzEgB"
      },
      "outputs": [],
      "source": [
        "# Function for confusion matrix\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, labels=[\"Decrease\", \"Increase\"],\n",
        "                          normalize=False, title=None, cmap=plt.cm.coolwarm):\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    fig, ax = plt.subplots(figsize=(12,6))\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=labels, yticklabels=labels,\n",
        "           title=title,\n",
        "           ylabel='ACTUAL',\n",
        "           xlabel='PREDICTED')\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 1.5\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"snow\" if cm[i, j] > thresh else \"orange\",\n",
        "                    size=26)\n",
        "    ax.grid(False)\n",
        "    fig.tight_layout()\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv_hWk8UrZ_z"
      },
      "source": [
        "### Create dictionary of classifiers to train and predict on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KblCby8KxZ8J"
      },
      "outputs": [],
      "source": [
        "# test models\n",
        "models = {  'LinearDiscriminantAnalysis':LinearDiscriminantAnalysis(),\n",
        "            'SVM Classification': SVC(),\n",
        "            'SGDClassifier': SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=100),\n",
        "            'KNeighborsClassifier':KNeighborsClassifier(n_neighbors=10),\n",
        "            'GaussianProcessClassifier': GaussianProcessClassifier(),\n",
        "            'RandomForestClassifier': RandomForestClassifier(n_estimators=100)\n",
        "            }\n",
        "\n",
        "for model_name in models.keys():\n",
        "\n",
        "    model = models[model_name]\n",
        "    print('\\n'+'--------------',model_name,'---------------'+'\\n')\n",
        "    model.fit(X_train,y_train)\n",
        "    # Plot confusion matrix\n",
        "    plot_confusion_matrix(model.predict(X_test),y_test, title=\"Confusion Matrix\")\n",
        "    np.set_printoptions(precision=1)\n",
        "    plt.show()\n",
        "    # Final Classification Report\n",
        "    print(classification_report(model.predict(X_test),y_test, target_names=['Decrease', 'Increase']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ7mjNCVryRY"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "The precision score is the 'exactness', or ability of the model to return only relevant instances. When a model makes a prediction, how often it is correct?\n",
        "\n",
        "It appears that the model which correctly predicted the increase in price most often was the Random Forest Classifier at 66%, and the K-Nearest Neighbours Classifier was best at predicting the decrease in price 63% of the time.\n",
        "\n",
        "None of the scores were particularly outstanding and further improvements might include updating the lexicon with words and sentiments from other more specialised sources such as the [Loughran-McDonald Financial Sentiment Word Lists](https://sraf.nd.edu/textual-analysis/resources/#LM%20Sentiment%20Word%20Lists). This would likely result in more accurate sentiment analysis as it was specifically built for financial text whereas VADER is more attuned to sentiments expressed in social media.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "2021-06-19 Sentiment analysis and Classifiers.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}